{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[assignment4-RNN机翻](http://web.stanford.edu/class/cs224n/assignments/a4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224n: Assignment #4\n",
    "\n",
    "## 1.  使用 RNN 进行神经机器翻译\n",
    "在机器翻译中，我们的目标是将句子从源语言（例如切诺基语）转换为目标语言（例如英语）。 在本次作业中，我们将实现一个带有注意力的序列到序列 (Seq2Seq) 网络，以构建神经机器翻译 (NMT) 系统。 在本节中，我们将描述所提出的 NMT 系统的训练过程，该系统使用双向 LSTM 编码器和单向 LSTM 解码器。\n",
    "\n",
    "### (g)\n",
    "\n",
    "&emsp;&emsp;First, the mask operation sets $e_t[src\\_len:]$ to negative infinity, i.e. the bit of 'pad' in $e_t$ becomes negative infinity. So we know the effect of mask operatin is to make the prob of 'pad' in the attention vector($\\alpha_t$ in the PDF) to be zero.\n",
    "&emsp;&emsp;If we don't apply mask operation, the decode will use the information  of 'pad' of hidden states, and $O_t$ maybe predicted as 'pad', that's what we don't expect.\n",
    "\n",
    "### (j)\n",
    "\n",
    "&emsp;&emsp;Dot product attention is computationally easy and directly, the disadvantage is too easy to get the true informations between $h^{dec}$ and $h^{enc}$.\n",
    "&emsp;&emsp;Multiplicative attention seems like a transition between dot product and additive attention. It is similar in dimentionality as additive attention, and it run fater and is more space-efficient than the latter in small dimentionality. But it shows declining trend in large dim.\n",
    "&emsp;&emsp;The disadvantage of additive attention is that we need more hyperparameters to be tuned such as $W_1, W_2$ and $V$ in the equation. Howerver, it can fit more complex situation and in experiment the additive attention always outperform the two others.\n",
    "\n",
    "## 2. Analyzing NMT Systems\n",
    "\n",
    "### (a)\n",
    "\n",
    "Sorry for my knowledge level, I can't give the real reason behind the intuition.\n",
    "$\\mathrm{i.}$ speciﬁc linguistic construct\n",
    "$\\mathrm{ii.}$ speciﬁc linguistic construct\n",
    "$\\mathrm{iii.}$ speciﬁc model limitations(We can't know the unkonw word)\n",
    "$\\mathrm{iv.}$ speciﬁc model limitations\n",
    "$\\mathrm{v.}$ speciﬁc model limitations(the model may pay more attetion to the word 'she', so the result is \"the women's\")\n",
    "$\\mathrm{vi.}$ speciﬁc model limitations(the model can't distinguish the difference between French unit and American unit)\n",
    "\n",
    "### (b)\n",
    "\n",
    "### (c)\n",
    "$\\mathrm{i.}$ For $\\mathbf{c}_1$:\n",
    "$\n",
    "p_1 = 0.6 \\\\\n",
    "p_2 = 0.75 \\\\\n",
    "r^* = 4 \\\\\n",
    "BP = 1 (c_1 > r^*) \\\\\n",
    "BLEU = 0.67 \\\\\n",
    "$\n",
    "for $\\mathbf{c}_2$:\n",
    "$\n",
    "p_1 = 1 \\\\\n",
    "p_2 = 0.5 \\\\\n",
    "r^* = 4 \\\\\n",
    "BP = 1 (c_1 > r^*) \\\\\n",
    "BLEU = 0.70\n",
    "$\n",
    "\n",
    "So $\\mathbf{c}_2$ get higher BLEU score. I agree with it.\n",
    "\n",
    "$\\mathrm{ii.}$ For $\\mathbf{c}_1$:\n",
    "$\n",
    "p_1 = 0.6 \\\\\n",
    "p_2 = 0.4 \\\\\n",
    "r^* = 6 \\\\\n",
    "BP = exp(-0.2) \\\\\n",
    "BLEU = 0.401 \\\\\n",
    "$\n",
    "for $\\mathbf{c}_2$:\n",
    "$\n",
    "p_1 = 0.4 \\\\\n",
    "p_2 = 0.2 \\\\\n",
    "r^* = 4 \\\\\n",
    "BP = epx(-0.2) \\\\\n",
    "BLEU = 0.23\n",
    "$\n",
    "\n",
    "So $\\mathbf{c}_1$ get higher BLEU score. In reality, $\\mathbf{c}_2$ is a better translation.\n",
    "    \n",
    "$\\mathrm{iii.}$ We can get the reason from the question above. A single reference translation make our candidate sentence particular, somehow, it's overfitting.\n",
    "\n",
    "$\\mathrm{iv.}$\n",
    "Advantages:\n",
    "- It's convenient and fast, a good substitue of human evaluation.\n",
    "- In the corpus level, the score of BLEU is close to human's score.\n",
    "  \n",
    "Disadvantages:\n",
    "- It only works well on the corpus level because any zeros in precision scores will zero the entire BLEU score.\n",
    "- BLEU score as presented suffers for only comparing a candidate translation against a single reference, which is surely a noisy representation of the relevant n-grams that need to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sanity Check for Question 1d: Encode\n",
      "--------------------------------------------------------------------------------\n",
      "enc_hiddens Sanity Checks Passed!\n",
      "dec_init_state[0] Sanity Checks Passed!\n",
      "dec_init_state[1] Sanity Checks Passed!\n",
      "--------------------------------------------------------------------------------\n",
      "All Sanity Checks Passed for Question 1d: Encode!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Running Sanity Check for Question 1e: Decode\n",
      "--------------------------------------------------------------------------------\n",
      "combined_outputs Sanity Checks Passed!\n",
      "--------------------------------------------------------------------------------\n",
      "All Sanity Checks Passed for Question 1e: Decode!\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Running Sanity Check for Question 1f: Step\n",
      "--------------------------------------------------------------------------------\n",
      "dec_state[0] Sanity Checks Passed!\n",
      "dec_state[1] Sanity Checks Passed!\n",
      "combined_output  Sanity Checks Passed!\n",
      "e_t Sanity Checks Passed!\n",
      "--------------------------------------------------------------------------------\n",
      "All Sanity Checks Passed for Question 1f: Step!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuyanjie\\Desktop\\deep_learning\\CS224n-2019-solutions-master\\assignments\\a4\\nmt_model.py:352: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:921.)\n",
      "  e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n"
     ]
    }
   ],
   "source": [
    "%run sanity_check.py 1d\n",
    "%run sanity_check.py 1e\n",
    "%run sanity_check.py 1f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-gpu]",
   "language": "python",
   "name": "conda-env-pytorch-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
